
1)let’s make sure all the packages are up to date. Type the following at the prompt and press Enter
sudo apt-get update
export PS1='\u:\W\$ '
2)Check to see if your Ubuntu Linux operating system architecture is 32-bit or 64-bit, open up a terminal and run the following command below. Type/Copy/Paste: file /sbin/init 

3)Installing Java :
sudo apt-add-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
sudo apt install oracle-java8-set-default
Java installation folder will be "/usr/lib/jvm"
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
echo $JAVA_HOME
All Java environment variables are set in /etc/profile.d/jdk.sh, which is a file installed by oracle-java8-set-default and is read by the shell on start-up

4)Install eclipse
http://www.krizna.com/ubuntu/install-eclipse-ubuntu-14-04/

5)Install Tomcat 9
http://tecadmin.net/install-tomcat-9-on-ubuntu/

cd /opt
sudo wget http://www-us.apache.org/dist/tomcat/tomcat-9/v9.0.0.M26/bin/apache-tomcat-9.0.0.M26.tar.gz
sudo tar xzf apache-tomcat-9.0.0.M26.tar.gz

Start : sudo /opt/tomcat/bin/startup.sh
Stop: sudo /opt/tomcat/bin/shutdown.sh

To check if it is running:
ps -ef | grep tomcat
tail -f logs/catalina.out
sudo wget localhost:8080

6)Hadoop
Sudo su hduser  (hduser)
hduser@anveshan-All-Series:/usr/local/hadoop/sbin$ ./start-all.sh
/usr/local/hadoop/sbin/start-all.sh
/usr/local/hadoop/sbin/stop-all.sh
Sudo su hduser
Hadoop fs -ls /
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/{loggedin user}
hdfs dfs -ls
Ex: hduser@anveshan-All-Series:/home/anveshan$ hdfs dfs -mkdir /user/hduser

6.1) If Data node is not starting :
That means you have cleared the metadata from namenode. Now the files which you have stored for running the word count are still in the datanode and datanode has no idea where to send the block reports since you formatted the namenode so it will not start. 
cat /usr/local/hadoop/etc/hadoop/hdfs-site.xml

This step is important, see where datanode's data is getting stored. It is the value associated for datanode.data.dir. For me it is /usr/local/hadoop/hadoop_data/hdfs/datanode. Open your terminal and navigate to above directory and delete the directory named current which will be there under that directory. Make sure you are only deleting the "current" directory.

sudo rm -r /usr/local/hadoop/hadoop_data/hdfs/datanode/current
hadoop namenode -format
Jps

2800 DataNode
3377 NodeManager
3219 ResourceManager

3748 Jps
2648 NameNode
3019 SecondaryNameNode

Hadoop commands and Notes:
Login to hduser for connecting to hadoop
Sudo su hduser  (hduser)
hduser@anveshan-All-Series:/usr/local/hadoop/sbin$ ./start-all.sh
/usr/local/hadoop/sbin/start-all.sh
/usr/local/hadoop/sbin/stop-all.sh
http://localhost:50070/
Jps -- to check what demons are running in the background
--Below steps to remove an error
Sudo su hduser
Hadoop fs -ls /  (if there is an error)
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/{loggedin user}
hdfs dfs -ls
Local directory : /user/anveshan/hdpl
Hadoop directory :/user/hduser
(in cloudera, /home/cloudera is local and /user/cloudera is hadoop FS)
--copyFromLocal
hadoop fs -copyFromLocal helloworld.txt /user/hduser/
Check the urls :
http://localhost:8088/cluster resource manager
http://localhost:50070/dfshealth.html#tab-overview Name node
http://localhost:50070/logs/  logs
http://localhost:50070/logs/userlogs/  (check the application from the console)
hdfs fsck spark/opSpark/part-00001 -files -blocks -locations
-> We can find all config files under /etc/hadoop/conf/  
   In my system it is /usr/local/hadoop/etc/hadoop/
-> NameNode IP address can be found from "Core-site.xml" with property fs.defaultFS


7)Running a map reduce :
•If we are creating a executable jar, then we don’t have to mention class name
/home/anveshan$ hadoop jar /home/anveshan/hdpl/progs/wc1.jar mrprogs/wc1/ip mrprogs/wc1/op1
hadoop fs -cat /user/hduser/mrprogs/wordCount/op_cins/part-r-00000

8)Install  virtual box
http://tecadmin.net/install-oracle-virtualbox-on-ubuntu/
Virtualbox

Install Spark :
https://spark.apache.org/downloads.html
Download spark 
sudo tar xvf /usr/local/spark-2.0.1-bin-hadoop2.7.tgz
sudo mv spark-2.0.1-bin-hadoop2.7 spark
Edit  bashrc to include SPARk_HOME=/usr/local/spark
If in hduser, then got root of user folder  ..do cd ~
hduser@anveshan-All-Series:/$ sudo vi .bashrc
Or 
sudo gedit .bashrc
Use the following command for sourcing the ~/.bashrc file.
$source .bashrc

Install Scala
wget http://www.scala-lang.org/files/archive/scala-2.10.4.tgz
sudo tar xvf scala-2.10.4.tgz
sudo mv scala-2.10.4 scala
-> to modify bashrc file, better do cd ~ and then sudo gedit .bashrc
sudo gedit ~/.bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
Check scala
scala
Scala -version (2.10.4)

Spark :
Spark-shell
Sc.version (2.0.1)
Give this permission to hduser
sudo chown -R hduser /usr/local/spark

To start master in spark :
/usr/local/spark/sbin/start-all.sh
/usr/local/spark/sbin/start-master.sh     -- This will run the spark server..tomcat
/usr/local/spark/sbin/stop-all.sh

Sample comands :
spark-shell
:quit

==================================================================
Running simple prog  in spark-shell :
sudo su hduser
/usr/local/spark/sbin/start-master.sh
/usr/local/spark/sbin/start-all.sh
cd /home/anveshan/spark
spark-shell(local mode : http://localhost:4040)
Create input file : ip.txt
val inputfile = sc.textFile("ip.txt")
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
counts.saveAsTextFile("output")
In another terminal,
cd /home/anveshan/spark/output
cat part-00000
:quit


Spark Programs :
val sampleFile = sc.textFile("sample.txt")
val linesWithSample = sampleFile.filter(line => line.contains("sample"))
linesWithSample.count()

Spark with hadoop word count :
spark-shell
val hadoopText = sc.textFile("hdfs://localhost:9000/user/hduser/spark/ip.txt")
val counts = hadoopText.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://localhost:9000/user/hduser/spark/opSpark")
In another terminal :
hadoop fs -cat spark/opSpark/part-00000
--Simple spark command to see if file exists or not
sc.textFile("/home/anveshan/spark/ip.txt").take(5).foreach(println)
sc.textFile("hdfs://localhost:9000/user/hduser/spark/ip.txt").take(5).foreach(println)

--> To run in yarn:
spark-shell --master yarn-client

==================================================================
Running SPARK PROGRAM via SBT
sudo su hduser
1.Create project in spark workspace : /home/anveshan/spark/sparkHello
2.Create sbt file :
name := "Spark Hello"
version := "1.0"
scalaVersion := "2.10.4"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.1"
3.Create structure : src/main/scala
4.In the scala folder,create scala class to be run
5.Go spark project in cmd (/home/anveshan/spark/sparkHello)
6.sbt
7.compile
8.package
9.Now exit from sbt
10. Run the spark engine :   /usr/local/spark/sbin/start-all.sh
11. Check if engine is running : http://anveshan-all-series:8080/
12. Now submit the jar for running:
$SPARK_HOME/bin/spark-submit --class "SparkHello" --master spark://anveshan-All-Series:7077 /home/anveshan/spark/sparkHello/target/scala-2.10/spark-hello_2.10-1.0.jar
13. Stop spark : /usr/local/spark/sbin/stop-all.sh

Use
sbt compile
sbt package

Another way :
1.Start spark server
2.spark-shell --master spark://anveshan-All-Series:7077
3.open http://localhost:8080 and see that in the spark applications, spark-shell is running
4.  val v1 = sc.parallelize(1 to 1000)
5.  val v2 = sc.parallelize(1001 to 1000000)
6.  val v3 = v1.union(v2)
http://localhost:4040

http://www.infoobjects.com/spark-submit-with-sbt/
http://blog.matthewrathbone.com/2013/01/05/a-quick-guide-to-hadoop-map-reduce-frameworks.html
Running Scala  progs
============================================
# sbt installation

echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get update
sudo apt-get install sbt
-- To check the version of sbt
sbt sbt-version

1.Go to Scala workplace and then to project(specific project you have written)
2.sbt
3.write one build.sbt
4.write *.scala which you program
5.run
6.select what you want to run

--Another way of running sbt with command line args
sbt "run-main Hello try"   -- run-main is command option,Hello is object name,try - is CLI arg

$ wget https://raw.github.com/snim2/gedit-scala-plugin/master/install.sh
$ chmod +x install.sh
$ ./install.sh
============================================
https://www.youtube.com/watch?v=OMo--kfkjyU&list=PLbu9W4c-C0iB--RNIzTXVksxQEI4vLK5-&index=8
https://github.com/dgadiraju/code/blob/master/hadoop/edw/cloudera/sqoop/sqoop_demo.txt
https://hadooptutorial.wikispaces.com/Custom+partitioner
============================================

This example illustrates how to use customized partitioner in a MapReduce program. The partitioning phase takes place after the map phase and before the reduce phase. The number of partitions is equal to the number of reducers. The data gets partitioned across the reducers according to the partitioning function[1] . The difference between a partitioner and a combiner is that the partitioner divides the data according to the number of reducers so that all the data in a single partition gets executed by a single reducer. However, the combiner functions similar to the reducer and processes the data in each partition. The combiner is an optimization to the reducer. The default partitioning function is the hash partitioning function where the hashing is done on the key. However it might be useful to partition the data according to some other function of the key or the value.
============================================
Mysql set up
sudo apt-get update
sudo apt-get install mysql-server
root/anvesh
/usr/bin/mysql_secure_installation
sudo iptables -I INPUT -p tcp --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
sudo iptables -I OUTPUT -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT
sudo service mysql start

mysql shell
/usr/bin/mysql -u root -p
anvesh

GIT
NOtes:
sudo apt-get install git

git config --global user.name "Anveshan3175"
git config --global user.email "akunduru4@gmail.com"

Install Python in Anaconda ubuntu
============================================
1. https://www.continuum.io/downloads
2. Download the installable in the downloads folder
3. Run this command : bash ~/Downloads/Anaconda3-4.3.1-Linux-x86_64.sh
4. Agree the license
5. Give the installation folder as default (just click enter when it is asked)
6. Accept appending version number
7. Launch : spyder
8. python --version
9. launch notebook : jupyter notebook
10. exit notebook : ctrl c
11. ctrl + i to see documentation
Beautiful Soup
sudo apt-get install python3-bs4


Install ubuntu on virtualbox
=============================
1)INstall virtualbox
2)Download ubunti iso file
3)Install it like in : https://www.youtube.com/watch?v=U2LrFJnSVWU

To see full view mode, Right Ctrl + F (Control button on right hand + F)
java9/Nan123du

Install java 9
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
sudo apt install oracle-java9-installer
sudo apt install oracle-java9-set-default
Test : java -version
set java home:
sudo update-alternatives --config java   ## Find teh path of java installed
sudo gedit /etc/environment  ## Ususally JAVA_HOME or any other system variables are set here
export JAVA_HOME=/usr/lib/jvm/java-9-oracle  ## Add at the bottom
echo $JAVA_HOME
Sometimes you may have to make entry in  ~/.bashrc is entry in /etc/environment is not working
sudo gedit ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-9-oracle ## Add this only if entry in /etc/environment is not working

Eclipse install:
sudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make
sudo apt update
sudo apt install ubuntu-make
umake ide eclipse-jee ## For java eclipse umake ide eclipse
select installation as /home/java9/eclipse-jee

Zoom install:
===============
Download latest zoom_2.0.52458.0531_amd64.deb package from https://zoom.us/download
bash> sudo dpkg -i zoom_2.0.52458.0531_amd64.deb
bash> sudo apt-get -f install
bash> dpkg -l |grep zoom
bash> zoom

cd /home/anveshan/zoom/
sudo dpkg -i zoom_amd64.deb
sudo apt-get -f install
dpkg -l |grep zoom
zoom

cd /home/anveshan/zoom/
zoom
-----
